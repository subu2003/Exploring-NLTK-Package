{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d5c1fc",
   "metadata": {},
   "source": [
    "# Natural Language Processing CSE4022 DA1\n",
    "###                                                                                                                      SUBRAMANIAN NACHIAPPAN 20BCE1019\n",
    "### Date: 16/01/2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6625ab",
   "metadata": {},
   "source": [
    "### 1. Utilize Python NLTK (Natural Language Tool Kit) Platform and do the following.\n",
    "\n",
    "### Install relevant Packages and Libraries\n",
    "\n",
    "- Explore Brown Corpus and find the size, tokens, categories,\n",
    "- Find the size of word tokens?\n",
    "- Find the size of word types?\n",
    "- Find the size of the category “government”\n",
    "- List the most frequent tokens\n",
    "- Count the number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55b42443",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e791298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 1161192\n"
     ]
    }
   ],
   "source": [
    "# Size of brown corpus\n",
    "corpus_size = len(brown.words())\n",
    "print(\"Corpus size:\", corpus_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b967cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 1439319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The/at',\n",
       " 'Fulton/np-tl',\n",
       " 'County/nn-tl',\n",
       " 'Grand/jj-tl',\n",
       " 'Jury/nn-tl',\n",
       " 'said/vbd',\n",
       " 'Friday/nr',\n",
       " 'an/at',\n",
       " 'investigation/nn',\n",
       " 'of/in',\n",
       " \"Atlanta's/np\",\n",
       " '$',\n",
       " 'recent/jj',\n",
       " 'primary/nn',\n",
       " 'election/nn',\n",
       " 'produced/vbd',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'no/at',\n",
       " 'evidence/nn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " 'that/cs',\n",
       " 'any/dti',\n",
       " 'irregularities/nns',\n",
       " 'took/vbd',\n",
       " 'place/nn',\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'jury/nn',\n",
       " 'further/rbr',\n",
       " 'said/vbd',\n",
       " 'in/in',\n",
       " 'term-end/nn',\n",
       " 'presentments/nns',\n",
       " 'that/cs',\n",
       " 'the/at',\n",
       " 'City/nn-tl',\n",
       " 'Executive/jj-tl',\n",
       " 'Committee/nn-tl',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'which/wdt',\n",
       " 'had/hvd',\n",
       " 'over-all/jj',\n",
       " 'charge/nn',\n",
       " 'of/in',\n",
       " 'the/at',\n",
       " 'election/nn',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'deserves/vbz',\n",
       " 'the/at',\n",
       " 'praise/nn',\n",
       " 'and/cc',\n",
       " 'thanks/nns',\n",
       " 'of/in',\n",
       " 'the/at',\n",
       " 'City/nn-tl',\n",
       " 'of/in-tl',\n",
       " 'Atlanta/np-tl',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " 'for/in',\n",
       " 'the/at',\n",
       " 'manner/nn',\n",
       " 'in/in',\n",
       " 'which/wdt',\n",
       " 'the/at',\n",
       " 'election/nn',\n",
       " 'was/bedz',\n",
       " 'conducted/vbn',\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'September-October/np',\n",
       " 'term/nn',\n",
       " 'jury/nn',\n",
       " 'had/hvd',\n",
       " 'been/ben',\n",
       " 'charged/vbn',\n",
       " 'by/in',\n",
       " 'Fulton/np-tl',\n",
       " 'Superior/jj-tl',\n",
       " 'Court/nn-tl',\n",
       " 'Judge/nn-tl',\n",
       " 'Durwood/np',\n",
       " 'Pye/np',\n",
       " 'to/to',\n",
       " 'investigate/vb',\n",
       " 'reports/nns',\n",
       " 'of/in',\n",
       " 'possible/jj',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'irregularities/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " 'in/in',\n",
       " 'the/at',\n",
       " 'hard-fought/jj',\n",
       " 'primary/nn',\n",
       " 'which/wdt',\n",
       " 'was/bedz',\n",
       " 'won/vbn',\n",
       " 'by/in',\n",
       " 'Mayor-nominate/nn-tl',\n",
       " 'Ivan/np',\n",
       " 'Allen/np',\n",
       " 'Jr./np',\n",
       " './',\n",
       " '.',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'Only/rb',\n",
       " 'a/at',\n",
       " 'relative/jj',\n",
       " 'handful/nn',\n",
       " 'of/in',\n",
       " 'such/jj',\n",
       " 'reports/nns',\n",
       " 'was/bedz',\n",
       " 'received/vbn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jury/nn',\n",
       " 'said/vbd',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'considering/in',\n",
       " 'the/at',\n",
       " 'widespread/jj',\n",
       " 'interest/nn',\n",
       " 'in/in',\n",
       " 'the/at',\n",
       " 'election/nn',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'number/nn',\n",
       " 'of/in',\n",
       " 'voters/nns',\n",
       " 'and/cc',\n",
       " 'the/at',\n",
       " 'size/nn',\n",
       " 'of/in',\n",
       " 'this/dt',\n",
       " 'city/nn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'jury/nn',\n",
       " 'said/vbd',\n",
       " 'it/pps',\n",
       " 'did/dod',\n",
       " 'find/vb',\n",
       " 'that/cs',\n",
       " 'many/ap',\n",
       " 'of/in',\n",
       " \"Georgia's/np\",\n",
       " '$',\n",
       " 'registration/nn',\n",
       " 'and/cc',\n",
       " 'election/nn',\n",
       " 'laws/nns',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'are/ber',\n",
       " 'outmoded/jj',\n",
       " 'or/cc',\n",
       " 'inadequate/jj',\n",
       " 'and/cc',\n",
       " 'often/rb',\n",
       " 'ambiguous/jj',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'It/pps',\n",
       " 'recommended/vbd',\n",
       " 'that/cs',\n",
       " 'Fulton/np',\n",
       " 'legislators/nns',\n",
       " 'act/vb',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'to/to',\n",
       " 'have/hv',\n",
       " 'these/dts',\n",
       " 'laws/nns',\n",
       " 'studied/vbn',\n",
       " 'and/cc',\n",
       " 'revised/vbn',\n",
       " 'to/in',\n",
       " 'the/at',\n",
       " 'end/nn',\n",
       " 'of/in',\n",
       " 'modernizing/vbg',\n",
       " 'and/cc',\n",
       " 'improving/vbg',\n",
       " 'them/ppo',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'grand/jj',\n",
       " 'jury/nn',\n",
       " 'commented/vbd',\n",
       " 'on/in',\n",
       " 'a/at',\n",
       " 'number/nn',\n",
       " 'of/in',\n",
       " 'other/ap',\n",
       " 'topics/nns',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'among/in',\n",
       " 'them/ppo',\n",
       " 'the/at',\n",
       " 'Atlanta/np',\n",
       " 'and/cc',\n",
       " 'Fulton/np-tl',\n",
       " 'County/nn-tl',\n",
       " 'purchasing/vbg',\n",
       " 'departments/nns',\n",
       " 'which/wdt',\n",
       " 'it/pps',\n",
       " 'said/vbd',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'are/ber',\n",
       " 'well/ql',\n",
       " 'operated/vbn',\n",
       " 'and/cc',\n",
       " 'follow/vb',\n",
       " 'generally/rb',\n",
       " 'accepted/vbn',\n",
       " 'practices/nns',\n",
       " 'which/wdt',\n",
       " 'inure/vb',\n",
       " 'to/in',\n",
       " 'the/at',\n",
       " 'best/jjt',\n",
       " 'interest/nn',\n",
       " 'of/in',\n",
       " 'both/abx',\n",
       " 'governments/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'Merger/nn-hl',\n",
       " 'proposed/vbn-hl',\n",
       " 'However/wrb',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jury/nn',\n",
       " 'said/vbd',\n",
       " 'it/pps',\n",
       " 'believes/vbz',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'these/dts',\n",
       " 'two/cd',\n",
       " 'offices/nns',\n",
       " 'should/md',\n",
       " 'be/be',\n",
       " 'combined/vbn',\n",
       " 'to/to',\n",
       " 'achieve/vb',\n",
       " 'greater/jjr',\n",
       " 'efficiency/nn',\n",
       " 'and/cc',\n",
       " 'reduce/vb',\n",
       " 'the/at',\n",
       " 'cost/nn',\n",
       " 'of/in',\n",
       " 'administration/nn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'City/nn-tl',\n",
       " 'Purchasing/vbg-tl',\n",
       " 'Department/nn-tl',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jury/nn',\n",
       " 'said/vbd',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'is/bez',\n",
       " 'lacking/vbg',\n",
       " 'in/in',\n",
       " 'experienced/vbn',\n",
       " 'clerical/jj',\n",
       " 'personnel/nns',\n",
       " 'as/cs',\n",
       " 'a/at',\n",
       " 'result/nn',\n",
       " 'of/in',\n",
       " 'city/nn',\n",
       " 'personnel/nns',\n",
       " 'policies/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'It/pps',\n",
       " 'urged/vbd',\n",
       " 'that/cs',\n",
       " 'the/at',\n",
       " 'city/nn',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'take/vb',\n",
       " 'steps/nns',\n",
       " 'to/to',\n",
       " 'remedy/vb',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " 'this/dt',\n",
       " 'problem/nn',\n",
       " './',\n",
       " '.',\n",
       " 'Implementation/nn',\n",
       " 'of/in',\n",
       " \"Georgia's/np\",\n",
       " '$',\n",
       " 'automobile/nn',\n",
       " 'title/nn',\n",
       " 'law/nn',\n",
       " 'was/bedz',\n",
       " 'also/rb',\n",
       " 'recommended/vbn',\n",
       " 'by/in',\n",
       " 'the/at',\n",
       " 'outgoing/jj',\n",
       " 'jury/nn',\n",
       " './',\n",
       " '.',\n",
       " 'It/pps',\n",
       " 'urged/vbd',\n",
       " 'that/cs',\n",
       " 'the/at',\n",
       " 'next/ap',\n",
       " 'Legislature/nn-tl',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'provide/vb',\n",
       " 'enabling/vbg',\n",
       " 'funds/nns',\n",
       " 'and/cc',\n",
       " 're-set/vb',\n",
       " 'the/at',\n",
       " 'effective/jj',\n",
       " 'date/nn',\n",
       " 'so/cs',\n",
       " 'that/cs',\n",
       " 'an/at',\n",
       " 'orderly/jj',\n",
       " 'implementation/nn',\n",
       " 'of/in',\n",
       " 'the/at',\n",
       " 'law/nn',\n",
       " 'may/md',\n",
       " 'be/be',\n",
       " 'effected/vbn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'grand/jj',\n",
       " 'jury/nn',\n",
       " 'took/vbd',\n",
       " 'a/at',\n",
       " 'swipe/nn',\n",
       " 'at/in',\n",
       " 'the/at',\n",
       " 'State/nn-tl',\n",
       " 'Welfare/nn-tl',\n",
       " \"Department's/nn\",\n",
       " '$',\n",
       " '-tl',\n",
       " 'handling/nn',\n",
       " 'of/in',\n",
       " 'federal/jj',\n",
       " 'funds/nns',\n",
       " 'granted/vbn',\n",
       " 'for/in',\n",
       " 'child/nn',\n",
       " 'welfare/nn',\n",
       " 'services/nns',\n",
       " 'in/in',\n",
       " 'foster/jj',\n",
       " 'homes/nns',\n",
       " './',\n",
       " '.',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'This/dt',\n",
       " 'is/bez',\n",
       " 'one/cd',\n",
       " 'of/in',\n",
       " 'the/at',\n",
       " 'major/jj',\n",
       " 'items/nns',\n",
       " 'in/in',\n",
       " 'the/at',\n",
       " 'Fulton/np-tl',\n",
       " 'County/nn-tl',\n",
       " 'general/jj',\n",
       " 'assistance/nn',\n",
       " 'program/nn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jury/nn',\n",
       " 'said/vbd',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'but/cc',\n",
       " 'the/at',\n",
       " 'State/nn-tl',\n",
       " 'Welfare/nn-tl',\n",
       " 'Department/nn-tl',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'has/hvz',\n",
       " 'seen/vbn',\n",
       " 'fit/jj',\n",
       " 'to/to',\n",
       " 'distribute/vb',\n",
       " 'these/dts',\n",
       " 'funds/nns',\n",
       " 'through/in',\n",
       " 'the/at',\n",
       " 'welfare/nn',\n",
       " 'departments/nns',\n",
       " 'of/in',\n",
       " 'all/abn',\n",
       " 'the/at',\n",
       " 'counties/nns',\n",
       " 'in/in',\n",
       " 'the/at',\n",
       " 'state/nn',\n",
       " 'with/in',\n",
       " 'the/at',\n",
       " 'exception/nn',\n",
       " 'of/in',\n",
       " 'Fulton/np-tl',\n",
       " 'County/nn-tl',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'which/wdt',\n",
       " 'receives/vbz',\n",
       " 'none/pn',\n",
       " 'of/in',\n",
       " 'this/dt',\n",
       " 'money/nn',\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'jurors/nns',\n",
       " 'said/vbd',\n",
       " 'they/ppss',\n",
       " 'realize/vb',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'a/at',\n",
       " 'proportionate/jj',\n",
       " 'distribution/nn',\n",
       " 'of/in',\n",
       " 'these/dts',\n",
       " 'funds/nns',\n",
       " 'might/md',\n",
       " 'disable/vb',\n",
       " 'this/dt',\n",
       " 'program/nn',\n",
       " 'in/in',\n",
       " 'our/pp',\n",
       " '$',\n",
       " 'less/ql',\n",
       " 'populous/jj',\n",
       " 'counties/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'Nevertheless/rb',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'we/ppss',\n",
       " 'feel/vb',\n",
       " 'that/cs',\n",
       " 'in/in',\n",
       " 'the/at',\n",
       " 'future/nn',\n",
       " 'Fulton/np-tl',\n",
       " 'County/nn-tl',\n",
       " 'should/md',\n",
       " 'receive/vb',\n",
       " 'some/dti',\n",
       " 'portion/nn',\n",
       " 'of/in',\n",
       " 'these/dts',\n",
       " 'available/jj',\n",
       " 'funds/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jurors/nns',\n",
       " 'said/vbd',\n",
       " './',\n",
       " '.',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'Failure/nn',\n",
       " 'to/to',\n",
       " 'do/do',\n",
       " 'this/dt',\n",
       " 'will/md',\n",
       " 'continue/vb',\n",
       " 'to/to',\n",
       " 'place/vb',\n",
       " 'a/at',\n",
       " 'disproportionate/jj',\n",
       " 'burden/nn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " 'on/in',\n",
       " 'Fulton/np',\n",
       " 'taxpayers/nns',\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'jury/nn',\n",
       " 'also/rb',\n",
       " 'commented/vbd',\n",
       " 'on/in',\n",
       " 'the/at',\n",
       " 'Fulton/np',\n",
       " \"ordinary's/nn\",\n",
       " '$',\n",
       " 'court/nn',\n",
       " 'which/wdt',\n",
       " 'has/hvz',\n",
       " 'been/ben',\n",
       " 'under/in',\n",
       " 'fire/nn',\n",
       " 'for/in',\n",
       " 'its/pp',\n",
       " '$',\n",
       " 'practices/nns',\n",
       " 'in/in',\n",
       " 'the/at',\n",
       " 'appointment/nn',\n",
       " 'of/in',\n",
       " 'appraisers/nns',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'guardians/nns',\n",
       " 'and/cc',\n",
       " 'administrators/nns',\n",
       " 'and/cc',\n",
       " 'the/at',\n",
       " 'awarding/nn',\n",
       " 'of/in',\n",
       " 'fees/nns',\n",
       " 'and/cc',\n",
       " 'compensation/nn',\n",
       " './',\n",
       " '.',\n",
       " 'Wards/nns-hl',\n",
       " 'protected/vbn-hl',\n",
       " 'The/at',\n",
       " 'jury/nn',\n",
       " 'said/vbd',\n",
       " 'it/pps',\n",
       " 'found/vbd',\n",
       " 'the/at',\n",
       " 'court/nn',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'has/hvz',\n",
       " 'incorporated/vbn',\n",
       " 'into/in',\n",
       " 'its/pp',\n",
       " '$',\n",
       " 'operating/vbg',\n",
       " 'procedures/nns',\n",
       " 'the/at',\n",
       " 'recommendations/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " 'of/in',\n",
       " 'two/cd',\n",
       " 'previous/jj',\n",
       " 'grand/jj',\n",
       " 'juries/nns',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'Atlanta/np-tl',\n",
       " 'Bar/nn-tl',\n",
       " 'Association/nn-tl',\n",
       " 'and/cc',\n",
       " 'an/at',\n",
       " 'interim/nn',\n",
       " 'citizens/nns',\n",
       " 'committee/nn',\n",
       " './',\n",
       " '.',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'These/dts',\n",
       " 'actions/nns',\n",
       " 'should/md',\n",
       " 'serve/vb',\n",
       " 'to/to',\n",
       " 'protect/vb',\n",
       " 'in/in',\n",
       " 'fact/nn',\n",
       " 'and/cc',\n",
       " 'in/in',\n",
       " 'effect/nn',\n",
       " 'the/at',\n",
       " \"court's/nn\",\n",
       " '$',\n",
       " 'wards/nns',\n",
       " 'from/in',\n",
       " 'undue/jj',\n",
       " 'costs/nns',\n",
       " 'and/cc',\n",
       " 'its/pp',\n",
       " '$',\n",
       " 'appointed/vbn',\n",
       " 'and/cc',\n",
       " 'elected/vbn',\n",
       " 'servants/nns',\n",
       " 'from/in',\n",
       " 'unmeritorious/jj',\n",
       " 'criticisms/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jury/nn',\n",
       " 'said/vbd',\n",
       " './',\n",
       " '.',\n",
       " 'Regarding/in',\n",
       " \"Atlanta's/np\",\n",
       " '$',\n",
       " 'new/jj',\n",
       " 'multi-million-dollar/jj',\n",
       " 'airport/nn',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jury/nn',\n",
       " 'recommended/vbd',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'that/cs',\n",
       " 'when/wrb',\n",
       " 'the/at',\n",
       " 'new/jj',\n",
       " 'management/nn',\n",
       " 'takes/vbz',\n",
       " 'charge/nn',\n",
       " 'Jan./np',\n",
       " '1/cd',\n",
       " 'the/at',\n",
       " 'airport/nn',\n",
       " 'be/be',\n",
       " 'operated/vbn',\n",
       " 'in/in',\n",
       " 'a/at',\n",
       " 'manner/nn',\n",
       " 'that/wps',\n",
       " 'will/md',\n",
       " 'eliminate/vb',\n",
       " 'political/jj',\n",
       " 'influences/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'jury/nn',\n",
       " 'did/dod',\n",
       " 'not/',\n",
       " '*',\n",
       " 'elaborate/vb',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'but/cc',\n",
       " 'it/pps',\n",
       " 'added/vbd',\n",
       " 'that/cs',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'there/ex',\n",
       " 'should/md',\n",
       " 'be/be',\n",
       " 'periodic/jj',\n",
       " 'surveillance/nn',\n",
       " 'of/in',\n",
       " 'the/at',\n",
       " 'pricing/vbg',\n",
       " 'practices/nns',\n",
       " 'of/in',\n",
       " 'the/at',\n",
       " 'concessionaires/nns',\n",
       " 'for/in',\n",
       " 'the/at',\n",
       " 'purpose/nn',\n",
       " 'of/in',\n",
       " 'keeping/vbg',\n",
       " 'the/at',\n",
       " 'prices/nns',\n",
       " 'reasonable/jj',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'Ask/vb-hl',\n",
       " 'jail/nn-hl',\n",
       " 'deputies/nns-hl',\n",
       " 'On/in',\n",
       " 'other/ap',\n",
       " 'matters/nns',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jury/nn',\n",
       " 'recommended/vbd',\n",
       " 'that/cs',\n",
       " ':',\n",
       " '/',\n",
       " ':',\n",
       " '(',\n",
       " '/',\n",
       " '(',\n",
       " '1/cd',\n",
       " ')',\n",
       " '/',\n",
       " ')',\n",
       " 'Four/cd',\n",
       " 'additional/jj',\n",
       " 'deputies/nns',\n",
       " 'be/be',\n",
       " 'employed/vbn',\n",
       " 'at/in',\n",
       " 'the/at',\n",
       " 'Fulton/np-tl',\n",
       " 'County/nn-tl',\n",
       " 'Jail/nn-tl',\n",
       " 'and/cc',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'a/at',\n",
       " 'doctor/nn',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'medical/jj',\n",
       " 'intern/nn',\n",
       " 'or/cc',\n",
       " 'extern/nn',\n",
       " 'be/be',\n",
       " 'employed/vbn',\n",
       " 'for/in',\n",
       " 'night/nn',\n",
       " 'and/cc',\n",
       " 'weekend/nn',\n",
       " 'duty/nn',\n",
       " 'at/in',\n",
       " 'the/at',\n",
       " 'jail/nn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " '(',\n",
       " '/',\n",
       " '(',\n",
       " '2/cd',\n",
       " ')',\n",
       " '/',\n",
       " ')',\n",
       " 'Fulton/np',\n",
       " 'legislators/nns',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'work/vb',\n",
       " 'with/in',\n",
       " 'city/nn',\n",
       " 'officials/nns',\n",
       " 'to/to',\n",
       " 'pass/vb',\n",
       " 'enabling/vbg',\n",
       " 'legislation/nn',\n",
       " 'that/wps',\n",
       " 'will/md',\n",
       " 'permit/vb',\n",
       " 'the/at',\n",
       " 'establishment/nn',\n",
       " 'of/in',\n",
       " 'a/at',\n",
       " 'fair/jj',\n",
       " 'and/cc',\n",
       " 'equitable/jj',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " 'pension/nn',\n",
       " 'plan/nn',\n",
       " 'for/in',\n",
       " 'city/nn',\n",
       " 'employes/nns',\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'jury/nn',\n",
       " 'praised/vbd',\n",
       " 'the/at',\n",
       " 'administration/nn',\n",
       " 'and/cc',\n",
       " 'operation/nn',\n",
       " 'of/in',\n",
       " 'the/at',\n",
       " 'Atlanta/np-tl',\n",
       " 'Police/nns-tl',\n",
       " 'Department/nn-tl',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'Fulton/np-tl',\n",
       " 'Tax/nn-tl',\n",
       " \"Commissioner's/nn\",\n",
       " '$',\n",
       " '-tl',\n",
       " 'Office/nn-tl',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'Bellwood/np',\n",
       " 'and/cc',\n",
       " 'Alpharetta/np',\n",
       " 'prison/nn',\n",
       " 'farms/nns',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'Grady/np-tl',\n",
       " 'Hospital/nn-tl',\n",
       " 'and/cc',\n",
       " 'the/at',\n",
       " 'Fulton/np-tl',\n",
       " 'Health/nn-tl',\n",
       " 'Department/nn-tl',\n",
       " './',\n",
       " '.',\n",
       " 'Mayor/nn-tl',\n",
       " 'William/np',\n",
       " 'B./np',\n",
       " 'Hartsfield/np',\n",
       " 'filed/vbd',\n",
       " 'suit/nn',\n",
       " 'for/in',\n",
       " 'divorce/nn',\n",
       " 'from/in',\n",
       " 'his/pp',\n",
       " '$',\n",
       " 'wife/nn',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'Pearl/np',\n",
       " 'Williams/np',\n",
       " 'Hartsfield/np',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'in/in',\n",
       " 'Fulton/np-tl',\n",
       " 'Superior/jj-tl',\n",
       " 'Court/nn-tl',\n",
       " 'Friday/nr',\n",
       " './',\n",
       " '.',\n",
       " 'His/pp',\n",
       " '$',\n",
       " 'petition/nn',\n",
       " 'charged/vbd',\n",
       " 'mental/jj',\n",
       " 'cruelty/nn',\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'couple/nn',\n",
       " 'was/bedz',\n",
       " 'married/vbn',\n",
       " 'Aug./np',\n",
       " '2/cd',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = np.word_tokenize(brown.raw())\n",
    "print(\"Number of Tokens:\", len(tokens))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "427232bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "# Categories in Brown corpus\n",
    "categories = brown.categories()\n",
    "print(\"Categories:\", categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3dd0699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word tokens: 1161192\n"
     ]
    }
   ],
   "source": [
    "# Find the size of word tokens\n",
    "word_tokens = len(brown.words())\n",
    "print(\"Size of word tokens:\", word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba39ff23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word types: 56057\n"
     ]
    }
   ],
   "source": [
    "# Find the size of word types\n",
    "word_types = len(set(brown.words()))\n",
    "print(\"Size of word types:\", word_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7379a3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the category 'government': 70117\n"
     ]
    }
   ],
   "source": [
    "# Find the size of the category \"government\"\n",
    "government_words = len(brown.words(categories=['government']))\n",
    "print(\"Size of the category 'government':\", government_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e998e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 most frequent tokens\n",
    "fdist = np.FreqDist(brown.words())\n",
    "top_tokens = fdist.most_common(10)\n",
    "print(\"Most frequent tokens:\", top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1adf2358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 57340\n"
     ]
    }
   ],
   "source": [
    "# Count the number of sentences\n",
    "sentences = len(brown.sents())\n",
    "print(\"Number of sentences:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b2d6f",
   "metadata": {},
   "source": [
    "### 2. Explore the corpora available in NLTK (any two) (02 Marks)\n",
    "- Raw corpus\n",
    "- POS tagged\n",
    "- Parsed\n",
    "- Multilingual aligned\n",
    "- Spoken language\n",
    "- Semantic tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a00aff83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring Raw Corpus - Gutenberg Corpus\n",
    "#      Raw Corpus: NLTK has several raw text corpora that can be used for natural language processing tasks such as text classification, language modeling, and text generation. An example of a raw text corpus in NLTK is the nltk.corpus.gutenberg corpus, which contains a collection of texts from Project Gutenberg, an online library of free e-books.\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ee36ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words :  2621613\n",
      "Number of sentences :  98552\n",
      "Number of paragraphs :  47887\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words : \", len(gutenberg.words()))\n",
    "print(\"Number of sentences : \", len(gutenberg.sents()))\n",
    "print(\"Number of paragraphs : \", len(gutenberg.paras()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01898c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85efae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba13397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']']], [['VOLUME', 'I']], ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.paras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eb1e803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg.Word.Len  Avg.Sent.Len  Avg.Wrd.Freq  FileName \n",
      "5               25               26        austen-emma.txt\n",
      "5               26               17        austen-persuasion.txt\n",
      "5               28               22        austen-sense.txt\n",
      "4               34               79        bible-kjv.txt\n",
      "5               19               5        blake-poems.txt\n",
      "4               19               14        bryant-stories.txt\n",
      "4               18               12        burgess-busterbrown.txt\n",
      "4               20               13        carroll-alice.txt\n",
      "5               20               12        chesterton-ball.txt\n",
      "5               23               11        chesterton-brown.txt\n",
      "5               18               11        chesterton-thursday.txt\n",
      "4               21               25        edgeworth-parents.txt\n",
      "5               26               15        melville-moby_dick.txt\n",
      "5               52               11        milton-paradise.txt\n",
      "4               12               9        shakespeare-caesar.txt\n",
      "4               12               8        shakespeare-hamlet.txt\n",
      "4               12               7        shakespeare-macbeth.txt\n",
      "5               36               12        whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg.Word.Len \",\"Avg.Sent.Len \",\"Avg.Wrd.Freq \",\"FileName \")\n",
    "for fileid in gutenberg.fileids(): \n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print(round(num_chars/num_words),\"             \", round(num_words/num_sents), \"             \", round(num_words/num_vocab), \"      \", fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af182d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring Parsed Corpus - Treebank Corpus\n",
    "# About Treebank corpus - The TreeBank Corpora is a POS tagged corpora which means that each word in the corpus has been labeled with its grammatical role, such as noun, verb, adjective, etc. The Treebank corpora provide a syntactic parse for each sentence. The NLTK data package includes a 10% sample of the Penn Treebank (in treebank), as well as the Sinica Treebank (in sinica_treebank).\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "treebank.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d159387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41d8edea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'], ['Mr.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N.V.', ',', 'the', 'Dutch', 'publishing', 'group', '.'], ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4ef21d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d475ab98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent tokens: [(',', 4885), ('the', 4045), ('.', 3828), ('of', 2319), ('to', 2164), ('a', 1878), ('in', 1572), ('and', 1511), ('*-1', 1123), ('0', 1099)]\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(treebank.words())\n",
    "top_tokens = fdist.most_common(10)\n",
    "print(\"Most frequent tokens:\", top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f9aeec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in treebank corpus :  199\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of files in treebank corpus : \",len(treebank.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef1a39de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg.Word.Len  Avg.Sent.Len  Avg.Wrd.Freq  FileName \n",
      "21               16               1        wsj_0001.mrg\n",
      "23               27               1        wsj_0002.mrg\n",
      "29               26               2        wsj_0003.mrg\n",
      "24               24               2        wsj_0004.mrg\n",
      "25               19               1        wsj_0005.mrg\n",
      "28               24               1        wsj_0006.mrg\n",
      "23               19               1        wsj_0007.mrg\n",
      "26               23               2        wsj_0008.mrg\n",
      "23               20               2        wsj_0009.mrg\n",
      "24               22               2        wsj_0010.mrg\n",
      "24               26               2        wsj_0011.mrg\n",
      "26               28               2        wsj_0012.mrg\n",
      "28               31               2        wsj_0013.mrg\n",
      "22               37               1        wsj_0014.mrg\n",
      "26               26               2        wsj_0015.mrg\n",
      "25               22               2        wsj_0016.mrg\n",
      "28               26               2        wsj_0017.mrg\n",
      "27               27               2        wsj_0018.mrg\n",
      "24               15               2        wsj_0019.mrg\n",
      "30               31               2        wsj_0020.mrg\n",
      "29               25               2        wsj_0021.mrg\n",
      "25               21               2        wsj_0022.mrg\n",
      "25               24               1        wsj_0023.mrg\n",
      "29               22               2        wsj_0024.mrg\n",
      "26               29               2        wsj_0025.mrg\n",
      "30               32               2        wsj_0026.mrg\n",
      "27               23               2        wsj_0027.mrg\n",
      "22               23               1        wsj_0028.mrg\n",
      "24               27               2        wsj_0029.mrg\n",
      "23               27               1        wsj_0030.mrg\n",
      "32               40               1        wsj_0031.mrg\n",
      "26               24               2        wsj_0032.mrg\n",
      "24               24               2        wsj_0033.mrg\n",
      "27               24               2        wsj_0034.mrg\n",
      "30               28               2        wsj_0035.mrg\n",
      "27               21               3        wsj_0036.mrg\n",
      "28               28               2        wsj_0037.mrg\n",
      "30               24               2        wsj_0038.mrg\n",
      "27               25               2        wsj_0039.mrg\n",
      "25               27               2        wsj_0040.mrg\n",
      "27               25               3        wsj_0041.mrg\n",
      "23               24               2        wsj_0042.mrg\n",
      "25               26               3        wsj_0043.mrg\n",
      "28               23               3        wsj_0044.mrg\n",
      "28               27               3        wsj_0045.mrg\n",
      "23               19               1        wsj_0046.mrg\n",
      "31               29               2        wsj_0047.mrg\n",
      "27               25               2        wsj_0048.mrg\n",
      "29               27               3        wsj_0049.mrg\n",
      "31               21               1        wsj_0050.mrg\n",
      "28               25               2        wsj_0051.mrg\n",
      "25               15               1        wsj_0052.mrg\n",
      "25               30               2        wsj_0053.mrg\n",
      "28               23               1        wsj_0054.mrg\n",
      "29               30               1        wsj_0055.mrg\n",
      "19               9               1        wsj_0056.mrg\n",
      "26               25               2        wsj_0057.mrg\n",
      "27               25               1        wsj_0058.mrg\n",
      "27               32               2        wsj_0059.mrg\n",
      "29               27               2        wsj_0060.mrg\n",
      "25               18               1        wsj_0061.mrg\n",
      "28               23               2        wsj_0062.mrg\n",
      "29               30               2        wsj_0063.mrg\n",
      "28               25               2        wsj_0064.mrg\n",
      "27               24               1        wsj_0065.mrg\n",
      "22               24               1        wsj_0066.mrg\n",
      "30               32               2        wsj_0067.mrg\n",
      "23               19               2        wsj_0068.mrg\n",
      "25               23               1        wsj_0069.mrg\n",
      "31               33               1        wsj_0070.mrg\n",
      "27               29               3        wsj_0071.mrg\n",
      "27               29               2        wsj_0072.mrg\n",
      "29               26               2        wsj_0073.mrg\n",
      "26               18               2        wsj_0074.mrg\n",
      "29               30               2        wsj_0075.mrg\n",
      "25               18               1        wsj_0076.mrg\n",
      "26               17               2        wsj_0077.mrg\n",
      "24               28               1        wsj_0078.mrg\n",
      "24               16               1        wsj_0079.mrg\n",
      "29               28               2        wsj_0080.mrg\n",
      "28               32               2        wsj_0081.mrg\n",
      "28               26               2        wsj_0082.mrg\n",
      "25               26               3        wsj_0083.mrg\n",
      "24               21               2        wsj_0084.mrg\n",
      "27               24               2        wsj_0085.mrg\n",
      "32               22               2        wsj_0086.mrg\n",
      "26               18               2        wsj_0087.mrg\n",
      "30               32               3        wsj_0088.mrg\n",
      "27               27               3        wsj_0089.mrg\n",
      "26               27               3        wsj_0090.mrg\n",
      "30               28               2        wsj_0091.mrg\n",
      "29               27               2        wsj_0092.mrg\n",
      "28               27               2        wsj_0093.mrg\n",
      "28               22               2        wsj_0094.mrg\n",
      "28               26               2        wsj_0095.mrg\n",
      "25               38               3        wsj_0096.mrg\n",
      "28               26               2        wsj_0097.mrg\n",
      "28               33               2        wsj_0098.mrg\n",
      "22               22               2        wsj_0099.mrg\n",
      "27               23               2        wsj_0100.mrg\n",
      "27               42               2        wsj_0101.mrg\n",
      "27               20               2        wsj_0102.mrg\n",
      "29               29               2        wsj_0103.mrg\n",
      "20               16               1        wsj_0104.mrg\n",
      "29               28               2        wsj_0105.mrg\n",
      "27               25               2        wsj_0106.mrg\n",
      "34               40               2        wsj_0107.mrg\n",
      "29               23               3        wsj_0108.mrg\n",
      "28               25               2        wsj_0109.mrg\n",
      "20               22               3        wsj_0110.mrg\n",
      "29               25               2        wsj_0111.mrg\n",
      "31               31               3        wsj_0112.mrg\n",
      "25               26               2        wsj_0113.mrg\n",
      "26               24               2        wsj_0114.mrg\n",
      "30               28               2        wsj_0115.mrg\n",
      "27               28               3        wsj_0116.mrg\n",
      "28               27               2        wsj_0117.mrg\n",
      "27               26               4        wsj_0118.mrg\n",
      "27               31               2        wsj_0119.mrg\n",
      "25               21               2        wsj_0120.mrg\n",
      "25               23               3        wsj_0121.mrg\n",
      "30               30               1        wsj_0122.mrg\n",
      "30               26               2        wsj_0123.mrg\n",
      "25               23               2        wsj_0124.mrg\n",
      "23               20               3        wsj_0125.mrg\n",
      "27               22               2        wsj_0126.mrg\n",
      "25               28               2        wsj_0127.mrg\n",
      "27               28               3        wsj_0128.mrg\n",
      "30               31               2        wsj_0129.mrg\n",
      "31               28               2        wsj_0130.mrg\n",
      "22               27               1        wsj_0131.mrg\n",
      "26               27               2        wsj_0132.mrg\n",
      "28               25               1        wsj_0133.mrg\n",
      "29               26               2        wsj_0134.mrg\n",
      "28               25               2        wsj_0135.mrg\n",
      "26               30               2        wsj_0136.mrg\n",
      "27               21               2        wsj_0137.mrg\n",
      "21               26               2        wsj_0138.mrg\n",
      "26               16               1        wsj_0139.mrg\n",
      "26               29               1        wsj_0140.mrg\n",
      "24               23               2        wsj_0141.mrg\n",
      "25               24               3        wsj_0142.mrg\n",
      "24               32               1        wsj_0143.mrg\n",
      "24               29               2        wsj_0144.mrg\n",
      "26               27               2        wsj_0145.mrg\n",
      "31               30               2        wsj_0146.mrg\n",
      "27               32               2        wsj_0147.mrg\n",
      "26               29               3        wsj_0148.mrg\n",
      "29               26               2        wsj_0149.mrg\n",
      "25               20               2        wsj_0150.mrg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29               31               2        wsj_0151.mrg\n",
      "24               23               2        wsj_0152.mrg\n",
      "24               26               2        wsj_0153.mrg\n",
      "26               29               2        wsj_0154.mrg\n",
      "29               26               3        wsj_0155.mrg\n",
      "32               29               2        wsj_0156.mrg\n",
      "25               29               2        wsj_0157.mrg\n",
      "26               25               2        wsj_0158.mrg\n",
      "28               25               2        wsj_0159.mrg\n",
      "27               32               2        wsj_0160.mrg\n",
      "30               32               2        wsj_0161.mrg\n",
      "27               25               2        wsj_0162.mrg\n",
      "28               25               2        wsj_0163.mrg\n",
      "27               27               2        wsj_0164.mrg\n",
      "27               29               2        wsj_0165.mrg\n",
      "28               32               2        wsj_0166.mrg\n",
      "25               24               2        wsj_0167.mrg\n",
      "24               30               2        wsj_0168.mrg\n",
      "24               25               2        wsj_0169.mrg\n",
      "29               22               2        wsj_0170.mrg\n",
      "26               23               2        wsj_0171.mrg\n",
      "28               26               2        wsj_0172.mrg\n",
      "26               29               2        wsj_0173.mrg\n",
      "29               25               2        wsj_0174.mrg\n",
      "26               24               2        wsj_0175.mrg\n",
      "28               33               2        wsj_0176.mrg\n",
      "26               21               2        wsj_0177.mrg\n",
      "25               20               2        wsj_0178.mrg\n",
      "26               23               2        wsj_0179.mrg\n",
      "28               26               2        wsj_0180.mrg\n",
      "27               30               2        wsj_0181.mrg\n",
      "26               24               2        wsj_0182.mrg\n",
      "26               29               2        wsj_0183.mrg\n",
      "28               24               2        wsj_0184.mrg\n",
      "23               13               1        wsj_0185.mrg\n",
      "28               26               2        wsj_0186.mrg\n",
      "26               26               2        wsj_0187.mrg\n",
      "28               30               2        wsj_0188.mrg\n",
      "23               25               2        wsj_0189.mrg\n",
      "25               17               1        wsj_0190.mrg\n",
      "26               27               1        wsj_0191.mrg\n",
      "27               25               3        wsj_0192.mrg\n",
      "28               26               1        wsj_0193.mrg\n",
      "27               28               2        wsj_0194.mrg\n",
      "28               31               1        wsj_0195.mrg\n",
      "25               44               1        wsj_0196.mrg\n",
      "29               54               1        wsj_0197.mrg\n",
      "29               32               2        wsj_0198.mrg\n",
      "28               15               1        wsj_0199.mrg\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg.Word.Len \",\"Avg.Sent.Len \",\"Avg.Wrd.Freq \",\"FileName \")\n",
    "for fileid in treebank.fileids(): \n",
    "    num_chars = len(treebank.raw(fileid))\n",
    "    num_words = len(treebank.words(fileid))\n",
    "    num_sents = len(treebank.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in treebank.words(fileid)))\n",
    "    print(round(num_chars/num_words),\"             \", round(num_words/num_sents), \"             \", round(num_words/num_vocab), \"      \", fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c9319",
   "metadata": {},
   "source": [
    "### 3.Create a text corpus with a minimum of 200 words (unique content). Implement the following text processing (05 Marks)\n",
    "- Word segmentation\n",
    "- Sentence segmentation\n",
    "- Convert to Lowercase\n",
    "- Stop words removal\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- Part of speech tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "787c7881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "text=\"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa9aad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words in the created text corpus :  160\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', 'such', 'as', 'WordNet', ',', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', ',', 'wrappers', 'for', 'industrial-strength', 'NLP', 'libraries', ',', 'and', 'an', 'active', 'discussion', 'forum.Thanks', 'to', 'a', 'hands-on', 'guide', 'introducing', 'programming', 'fundamentals', 'alongside', 'topics', 'in', 'computational', 'linguistics', ',', 'plus', 'comprehensive', 'API', 'documentation', ',', 'NLTK', 'is', 'suitable', 'for', 'linguists', ',', 'engineers', ',', 'students', ',', 'educators', ',', 'researchers', ',', 'and', 'industry', 'users', 'alike', '.', 'NLTK', 'is', 'available', 'for', 'Windows', ',', 'Mac', 'OS', 'X', ',', 'and', 'Linux', '.', 'Best', 'of', 'all', ',', 'NLTK', 'is', 'a', 'free', ',', 'open', 'source', ',', 'community-driven', 'project.NLTK', 'has', 'been', 'called', '“', 'a', 'wonderful', 'tool', 'for', 'teaching', ',', 'and', 'working', 'in', ',', 'computational', 'linguistics', 'using', 'Python', ',', '”', 'and', '“', 'an', 'amazing', 'library', 'to', 'play', 'with', 'natural', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word segmentation\n",
    "words = word_tokenize(text)\n",
    "print(\"Number of Words in the created text corpus : \",len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "078f1cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences :  4\n",
      "['NLTK is a leading platform for building Python programs to work with human language data.', 'It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.', 'NLTK is available for Windows, Mac OS X, and Linux.', 'Best of all, NLTK is a free, open source, community-driven project.NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence segmentation\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Number of Sentences : \",len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e04041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nltk', 'is', 'a', 'leading', 'platform', 'for', 'building', 'python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'it', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', 'such', 'as', 'wordnet', ',', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', ',', 'wrappers', 'for', 'industrial-strength', 'nlp', 'libraries', ',', 'and', 'an', 'active', 'discussion', 'forum.thanks', 'to', 'a', 'hands-on', 'guide', 'introducing', 'programming', 'fundamentals', 'alongside', 'topics', 'in', 'computational', 'linguistics', ',', 'plus', 'comprehensive', 'api', 'documentation', ',', 'nltk', 'is', 'suitable', 'for', 'linguists', ',', 'engineers', ',', 'students', ',', 'educators', ',', 'researchers', ',', 'and', 'industry', 'users', 'alike', '.', 'nltk', 'is', 'available', 'for', 'windows', ',', 'mac', 'os', 'x', ',', 'and', 'linux', '.', 'best', 'of', 'all', ',', 'nltk', 'is', 'a', 'free', ',', 'open', 'source', ',', 'community-driven', 'project.nltk', 'has', 'been', 'called', '“', 'a', 'wonderful', 'tool', 'for', 'teaching', ',', 'and', 'working', 'in', ',', 'computational', 'linguistics', 'using', 'python', ',', '”', 'and', '“', 'an', 'amazing', 'library', 'to', 'play', 'with', 'natural', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "# Convert to lowercase\n",
    "words = [word.lower() for word in words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "513dc2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Stop Words in English :  179\n",
      "Number of Words left after stop words removal :  118\n",
      "['nltk', 'leading', 'platform', 'building', 'python', 'programs', 'work', 'human', 'language', 'data', '.', 'provides', 'easy-to-use', 'interfaces', '50', 'corpora', 'lexical', 'resources', 'wordnet', ',', 'along', 'suite', 'text', 'processing', 'libraries', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'semantic', 'reasoning', ',', 'wrappers', 'industrial-strength', 'nlp', 'libraries', ',', 'active', 'discussion', 'forum.thanks', 'hands-on', 'guide', 'introducing', 'programming', 'fundamentals', 'alongside', 'topics', 'computational', 'linguistics', ',', 'plus', 'comprehensive', 'api', 'documentation', ',', 'nltk', 'suitable', 'linguists', ',', 'engineers', ',', 'students', ',', 'educators', ',', 'researchers', ',', 'industry', 'users', 'alike', '.', 'nltk', 'available', 'windows', ',', 'mac', 'os', 'x', ',', 'linux', '.', 'best', ',', 'nltk', 'free', ',', 'open', 'source', ',', 'community-driven', 'project.nltk', 'called', '“', 'wonderful', 'tool', 'teaching', ',', 'working', ',', 'computational', 'linguistics', 'using', 'python', ',', '”', '“', 'amazing', 'library', 'play', 'natural', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stop words removal\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(\"Number of Stop Words in English : \", len(stop_words))\n",
    "words = [word for word in words if word not in stop_words]\n",
    "print(\"Number of Words left after stop words removal : \", len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d5823ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nltk', 'lead', 'platform', 'build', 'python', 'program', 'work', 'human', 'languag', 'data', '.', 'provid', 'easy-to-us', 'interfac', '50', 'corpora', 'lexic', 'resourc', 'wordnet', ',', 'along', 'suit', 'text', 'process', 'librari', 'classif', ',', 'token', ',', 'stem', ',', 'tag', ',', 'pars', ',', 'semant', 'reason', ',', 'wrapper', 'industrial-strength', 'nlp', 'librari', ',', 'activ', 'discuss', 'forum.thank', 'hands-on', 'guid', 'introduc', 'program', 'fundament', 'alongsid', 'topic', 'comput', 'linguist', ',', 'plu', 'comprehens', 'api', 'document', ',', 'nltk', 'suitabl', 'linguist', ',', 'engin', ',', 'student', ',', 'educ', ',', 'research', ',', 'industri', 'user', 'alik', '.', 'nltk', 'avail', 'window', ',', 'mac', 'os', 'x', ',', 'linux', '.', 'best', ',', 'nltk', 'free', ',', 'open', 'sourc', ',', 'community-driven', 'project.nltk', 'call', '“', 'wonder', 'tool', 'teach', ',', 'work', ',', 'comput', 'linguist', 'use', 'python', ',', '”', '“', 'amaz', 'librari', 'play', 'natur', 'languag', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcaa8e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nltk',\n",
       " 'leading',\n",
       " 'platform',\n",
       " 'building',\n",
       " 'python',\n",
       " 'program',\n",
       " 'work',\n",
       " 'human',\n",
       " 'language',\n",
       " 'data',\n",
       " '.',\n",
       " 'provides',\n",
       " 'easy-to-use',\n",
       " 'interface',\n",
       " '50',\n",
       " 'corpus',\n",
       " 'lexical',\n",
       " 'resource',\n",
       " 'wordnet',\n",
       " ',',\n",
       " 'along',\n",
       " 'suite',\n",
       " 'text',\n",
       " 'processing',\n",
       " 'library',\n",
       " 'classification',\n",
       " ',',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'tagging',\n",
       " ',',\n",
       " 'parsing',\n",
       " ',',\n",
       " 'semantic',\n",
       " 'reasoning',\n",
       " ',',\n",
       " 'wrapper',\n",
       " 'industrial-strength',\n",
       " 'nlp',\n",
       " 'library',\n",
       " ',',\n",
       " 'active',\n",
       " 'discussion',\n",
       " 'forum.thanks',\n",
       " 'hands-on',\n",
       " 'guide',\n",
       " 'introducing',\n",
       " 'programming',\n",
       " 'fundamental',\n",
       " 'alongside',\n",
       " 'topic',\n",
       " 'computational',\n",
       " 'linguistics',\n",
       " ',',\n",
       " 'plus',\n",
       " 'comprehensive',\n",
       " 'api',\n",
       " 'documentation',\n",
       " ',',\n",
       " 'nltk',\n",
       " 'suitable',\n",
       " 'linguist',\n",
       " ',',\n",
       " 'engineer',\n",
       " ',',\n",
       " 'student',\n",
       " ',',\n",
       " 'educator',\n",
       " ',',\n",
       " 'researcher',\n",
       " ',',\n",
       " 'industry',\n",
       " 'user',\n",
       " 'alike',\n",
       " '.',\n",
       " 'nltk',\n",
       " 'available',\n",
       " 'window',\n",
       " ',',\n",
       " 'mac',\n",
       " 'o',\n",
       " 'x',\n",
       " ',',\n",
       " 'linux',\n",
       " '.',\n",
       " 'best',\n",
       " ',',\n",
       " 'nltk',\n",
       " 'free',\n",
       " ',',\n",
       " 'open',\n",
       " 'source',\n",
       " ',',\n",
       " 'community-driven',\n",
       " 'project.nltk',\n",
       " 'called',\n",
       " '“',\n",
       " 'wonderful',\n",
       " 'tool',\n",
       " 'teaching',\n",
       " ',',\n",
       " 'working',\n",
       " ',',\n",
       " 'computational',\n",
       " 'linguistics',\n",
       " 'using',\n",
       " 'python',\n",
       " ',',\n",
       " '”',\n",
       " '“',\n",
       " 'amazing',\n",
       " 'library',\n",
       " 'play',\n",
       " 'natural',\n",
       " 'language',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c6379ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nltk', 'RB'),\n",
       " ('leading', 'VBG'),\n",
       " ('platform', 'NN'),\n",
       " ('building', 'NN'),\n",
       " ('python', 'NN'),\n",
       " ('programs', 'NNS'),\n",
       " ('work', 'VBP'),\n",
       " ('human', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('provides', 'VBZ'),\n",
       " ('easy-to-use', 'JJ'),\n",
       " ('interfaces', 'NNS'),\n",
       " ('50', 'CD'),\n",
       " ('corpora', 'NNS'),\n",
       " ('lexical', 'JJ'),\n",
       " ('resources', 'NNS'),\n",
       " ('wordnet', 'NN'),\n",
       " (',', ','),\n",
       " ('along', 'IN'),\n",
       " ('suite', 'JJ'),\n",
       " ('text', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('libraries', 'NNS'),\n",
       " ('classification', 'NN'),\n",
       " (',', ','),\n",
       " ('tokenization', 'NN'),\n",
       " (',', ','),\n",
       " ('stemming', 'VBG'),\n",
       " (',', ','),\n",
       " ('tagging', 'VBG'),\n",
       " (',', ','),\n",
       " ('parsing', 'VBG'),\n",
       " (',', ','),\n",
       " ('semantic', 'JJ'),\n",
       " ('reasoning', 'NN'),\n",
       " (',', ','),\n",
       " ('wrappers', 'NNS'),\n",
       " ('industrial-strength', 'VBP'),\n",
       " ('nlp', 'JJ'),\n",
       " ('libraries', 'NNS'),\n",
       " (',', ','),\n",
       " ('active', 'JJ'),\n",
       " ('discussion', 'NN'),\n",
       " ('forum.thanks', 'NNS'),\n",
       " ('hands-on', 'JJ'),\n",
       " ('guide', 'NN'),\n",
       " ('introducing', 'VBG'),\n",
       " ('programming', 'VBG'),\n",
       " ('fundamentals', 'NNS'),\n",
       " ('alongside', 'IN'),\n",
       " ('topics', 'NNS'),\n",
       " ('computational', 'JJ'),\n",
       " ('linguistics', 'NNS'),\n",
       " (',', ','),\n",
       " ('plus', 'CC'),\n",
       " ('comprehensive', 'JJ'),\n",
       " ('api', 'NN'),\n",
       " ('documentation', 'NN'),\n",
       " (',', ','),\n",
       " ('nltk', 'CC'),\n",
       " ('suitable', 'JJ'),\n",
       " ('linguists', 'NNS'),\n",
       " (',', ','),\n",
       " ('engineers', 'NNS'),\n",
       " (',', ','),\n",
       " ('students', 'NNS'),\n",
       " (',', ','),\n",
       " ('educators', 'NNS'),\n",
       " (',', ','),\n",
       " ('researchers', 'NNS'),\n",
       " (',', ','),\n",
       " ('industry', 'NN'),\n",
       " ('users', 'NNS'),\n",
       " ('alike', 'RB'),\n",
       " ('.', '.'),\n",
       " ('nltk', 'CC'),\n",
       " ('available', 'JJ'),\n",
       " ('windows', 'NNS'),\n",
       " (',', ','),\n",
       " ('mac', 'FW'),\n",
       " ('os', 'FW'),\n",
       " ('x', 'FW'),\n",
       " (',', ','),\n",
       " ('linux', 'FW'),\n",
       " ('.', '.'),\n",
       " ('best', 'JJS'),\n",
       " (',', ','),\n",
       " ('nltk', 'JJ'),\n",
       " ('free', 'JJ'),\n",
       " (',', ','),\n",
       " ('open', 'JJ'),\n",
       " ('source', 'NN'),\n",
       " (',', ','),\n",
       " ('community-driven', 'JJ'),\n",
       " ('project.nltk', 'NN'),\n",
       " ('called', 'VBN'),\n",
       " ('“', 'RB'),\n",
       " ('wonderful', 'JJ'),\n",
       " ('tool', 'NN'),\n",
       " ('teaching', 'NN'),\n",
       " (',', ','),\n",
       " ('working', 'VBG'),\n",
       " (',', ','),\n",
       " ('computational', 'JJ'),\n",
       " ('linguistics', 'NNS'),\n",
       " ('using', 'VBG'),\n",
       " ('python', 'NN'),\n",
       " (',', ','),\n",
       " ('”', 'NNP'),\n",
       " ('“', 'NNP'),\n",
       " ('amazing', 'VBG'),\n",
       " ('library', 'JJ'),\n",
       " ('play', 'NN'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part of speech tagging\n",
    "pos_tagged_words = pos_tag(words)\n",
    "pos_tagged_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0134d608fe8e6084e5b5999b07506d2133d990d7a08e012442464fd241b0bf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
